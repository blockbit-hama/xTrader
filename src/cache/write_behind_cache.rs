use std::collections::{HashMap, HashSet};
use std::sync::Arc;
use std::time::{Duration, Instant};
use tokio::sync::{RwLock, Mutex};
use tokio::time::interval;
use log::{debug, error, info, warn};
use serde::{Serialize, Deserialize};

/// Write-Behind ìºì‹œ ì‹œìŠ¤í…œ
/// 
/// í•µì‹¬ ê¸°ëŠ¥:
/// 1. ë©”ëª¨ë¦¬ì—ì„œ ì¦‰ì‹œ ì½ê¸°/ì“°ê¸°
/// 2. ë°±ê·¸ë¼ìš´ë“œì—ì„œ DB ë™ê¸°í™”
/// 3. ë‹¤ì¸µ ìºì‹± (L1: ë©”ëª¨ë¦¬, L2: Redis, L3: DB)
/// 4. ìºì‹œ ë¬´íš¨í™” ë° ì¼ê´€ì„± ë³´ì¥
pub struct WriteBehindCache {
    /// L1 ìºì‹œ: ë©”ëª¨ë¦¬ (ê°€ì¥ ë¹ ë¦„)
    l1_cache: Arc<RwLock<HashMap<String, CacheEntry>>>,
    
    /// L2 ìºì‹œ: Redis (ì¤‘ê°„ ì†ë„)
    l2_cache: Option<Arc<dyn CacheLayer>>,
    
    /// L3 ìºì‹œ: DB (ê°€ì¥ ëŠë¦¼)
    l3_cache: Arc<dyn CacheLayer>,
    
    /// ë³€ê²½ëœ í‚¤ë“¤ ì¶”ì 
    dirty_keys: Arc<Mutex<HashSet<String>>>,
    
    /// ë™ê¸°í™” ê°„ê²©
    sync_interval: Duration,
    
    /// ìºì‹œ TTL
    cache_ttl: Duration,
    
    /// ì„±ëŠ¥ ë©”íŠ¸ë¦­
    metrics: Arc<CacheMetrics>,
    
    /// ë™ê¸°í™” ì‘ì—…ì í•¸ë“¤
    sync_worker: Option<tokio::task::JoinHandle<()>>,
}

/// ìºì‹œ ì—”íŠ¸ë¦¬
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct CacheEntry {
    pub value: serde_json::Value,
    pub timestamp: Instant,
    pub version: u64,
    pub ttl: Duration,
}

impl CacheEntry {
    pub fn new(value: serde_json::Value, ttl: Duration) -> Self {
        Self {
            value,
            timestamp: Instant::now(),
            version: 0,
            ttl,
        }
    }
    
    pub fn is_expired(&self) -> bool {
        self.timestamp.elapsed() > self.ttl
    }
    
    pub fn update(&mut self, value: serde_json::Value) {
        self.value = value;
        self.timestamp = Instant::now();
        self.version += 1;
    }
}

/// ìºì‹œ ë ˆì´ì–´ íŠ¸ë ˆì´íŠ¸
#[async_trait::async_trait]
pub trait CacheLayer: Send + Sync {
    async fn get(&self, key: &str) -> Result<Option<serde_json::Value>, CacheError>;
    async fn set(&self, key: &str, value: &serde_json::Value) -> Result<(), CacheError>;
    async fn delete(&self, key: &str) -> Result<(), CacheError>;
    async fn exists(&self, key: &str) -> Result<bool, CacheError>;
}

/// ìºì‹œ ì—ëŸ¬ íƒ€ì…
#[derive(Debug, thiserror::Error)]
pub enum CacheError {
    #[error("ìºì‹œ í‚¤ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŒ: {0}")]
    KeyNotFound(String),
    #[error("ìºì‹œ ë§Œë£Œë¨: {0}")]
    Expired(String),
    #[error("ìºì‹œ ë ˆì´ì–´ ì˜¤ë¥˜: {0}")]
    LayerError(String),
    #[error("ì§ë ¬í™” ì˜¤ë¥˜: {0}")]
    SerializationError(#[from] serde_json::Error),
}

/// ìºì‹œ ì„±ëŠ¥ ë©”íŠ¸ë¦­
#[derive(Debug, Default)]
pub struct CacheMetrics {
    pub l1_hits: std::sync::atomic::AtomicU64,
    pub l1_misses: std::sync::atomic::AtomicU64,
    pub l2_hits: std::sync::atomic::AtomicU64,
    pub l2_misses: std::sync::atomic::AtomicU64,
    pub l3_hits: std::sync::atomic::AtomicU64,
    pub l3_misses: std::sync::atomic::AtomicU64,
    pub sync_operations: std::sync::atomic::AtomicU64,
    pub sync_errors: std::sync::atomic::AtomicU64,
    pub start_time: Instant,
}

impl CacheMetrics {
    pub fn new() -> Self {
        Self {
            start_time: Instant::now(),
            ..Default::default()
        }
    }
    
    pub fn record_l1_hit(&self) {
        self.l1_hits.fetch_add(1, std::sync::atomic::Ordering::Relaxed);
    }
    
    pub fn record_l1_miss(&self) {
        self.l1_misses.fetch_add(1, std::sync::atomic::Ordering::Relaxed);
    }
    
    pub fn record_l2_hit(&self) {
        self.l2_hits.fetch_add(1, std::sync::atomic::Ordering::Relaxed);
    }
    
    pub fn record_l2_miss(&self) {
        self.l2_misses.fetch_add(1, std::sync::atomic::Ordering::Relaxed);
    }
    
    pub fn record_l3_hit(&self) {
        self.l3_hits.fetch_add(1, std::sync::atomic::Ordering::Relaxed);
    }
    
    pub fn record_l3_miss(&self) {
        self.l3_misses.fetch_add(1, std::sync::atomic::Ordering::Relaxed);
    }
    
    pub fn record_sync_success(&self) {
        self.sync_operations.fetch_add(1, std::sync::atomic::Ordering::Relaxed);
    }
    
    pub fn record_sync_error(&self) {
        self.sync_errors.fetch_add(1, std::sync::atomic::Ordering::Relaxed);
    }
    
    pub fn get_l1_hit_rate(&self) -> f64 {
        let hits = self.l1_hits.load(std::sync::atomic::Ordering::Relaxed);
        let misses = self.l1_misses.load(std::sync::atomic::Ordering::Relaxed);
        let total = hits + misses;
        if total > 0 {
            hits as f64 / total as f64 * 100.0
        } else {
            0.0
        }
    }
    
    pub fn get_overall_hit_rate(&self) -> f64 {
        let total_hits = self.l1_hits.load(std::sync::atomic::Ordering::Relaxed) +
                        self.l2_hits.load(std::sync::atomic::Ordering::Relaxed) +
                        self.l3_hits.load(std::sync::atomic::Ordering::Relaxed);
        let total_misses = self.l1_misses.load(std::sync::atomic::Ordering::Relaxed) +
                          self.l2_misses.load(std::sync::atomic::Ordering::Relaxed) +
                          self.l3_misses.load(std::sync::atomic::Ordering::Relaxed);
        let total = total_hits + total_misses;
        if total > 0 {
            total_hits as f64 / total as f64 * 100.0
        } else {
            0.0
        }
    }
}

impl WriteBehindCache {
    /// ìƒˆ Write-Behind ìºì‹œ ìƒì„±
    pub fn new(
        l3_cache: Arc<dyn CacheLayer>,
        sync_interval: Duration,
        cache_ttl: Duration,
    ) -> Self {
        Self {
            l1_cache: Arc::new(RwLock::new(HashMap::new())),
            l2_cache: None,
            l3_cache,
            dirty_keys: Arc::new(Mutex::new(HashSet::new())),
            sync_interval,
            cache_ttl,
            metrics: Arc::new(CacheMetrics::new()),
            sync_worker: None,
        }
    }
    
    /// L2 ìºì‹œ ì„¤ì • (Redis ë“±)
    pub fn with_l2_cache(mut self, l2_cache: Arc<dyn CacheLayer>) -> Self {
        self.l2_cache = Some(l2_cache);
        self
    }
    
    /// ìºì‹œ ì‹œì‘ (ë°±ê·¸ë¼ìš´ë“œ ë™ê¸°í™” ì‹œì‘)
    pub fn start(&mut self) {
        let l1_cache = self.l1_cache.clone();
        let l2_cache = self.l2_cache.clone();
        let l3_cache = self.l3_cache.clone();
        let dirty_keys = self.dirty_keys.clone();
        let metrics = self.metrics.clone();
        let sync_interval = self.sync_interval;
        
        let sync_worker = tokio::spawn(async move {
            let mut interval = interval(sync_interval);
            
            loop {
                interval.tick().await;
                
                // ë§Œë£Œëœ ìºì‹œ ì—”íŠ¸ë¦¬ ì •ë¦¬
                Self::cleanup_expired_entries(&l1_cache).await;
                
                // ë³€ê²½ëœ í‚¤ë“¤ ë™ê¸°í™”
                Self::sync_dirty_keys(&l1_cache, &l2_cache, &l3_cache, &dirty_keys, &metrics).await;
            }
        });
        
        self.sync_worker = Some(sync_worker);
        info!("ğŸš€ Write-Behind ìºì‹œ ì‹œì‘ë¨ (ë™ê¸°í™” ê°„ê²©: {:?})", sync_interval);
    }
    
    /// ìºì‹œ ì¤‘ì§€
    pub async fn stop(&mut self) {
        if let Some(worker) = self.sync_worker.take() {
            worker.abort();
            info!("Write-Behind ìºì‹œ ì¤‘ì§€ë¨");
        }
    }
    
    /// ìºì‹œì—ì„œ ê°’ ì¡°íšŒ (ë‹¤ì¸µ ìºì‹œ)
    pub async fn get<T>(&self, key: &str) -> Result<Option<T>, CacheError>
    where
        T: serde::de::DeserializeOwned,
    {
        // L1 ìºì‹œ í™•ì¸ (ë©”ëª¨ë¦¬)
        if let Some(entry) = self.l1_cache.read().await.get(key) {
            if !entry.is_expired() {
                self.metrics.record_l1_hit();
                return Ok(Some(serde_json::from_value(entry.value.clone())?));
            } else {
                // ë§Œë£Œëœ ì—”íŠ¸ë¦¬ ì œê±°
                self.l1_cache.write().await.remove(key);
            }
        }
        self.metrics.record_l1_miss();
        
        // L2 ìºì‹œ í™•ì¸ (Redis)
        if let Some(ref l2_cache) = self.l2_cache {
            if let Ok(Some(value)) = l2_cache.get(key).await {
                self.metrics.record_l2_hit();
                
                // L1 ìºì‹œì— ì €ì¥
                let entry = CacheEntry::new(value.clone(), self.cache_ttl);
                self.l1_cache.write().await.insert(key.to_string(), entry);
                
                return Ok(Some(serde_json::from_value(value)?));
            }
            self.metrics.record_l2_miss();
        }
        
        // L3 ìºì‹œ í™•ì¸ (DB)
        if let Ok(Some(value)) = self.l3_cache.get(key).await {
            self.metrics.record_l3_hit();
            
            // L1, L2 ìºì‹œì— ì €ì¥
            let entry = CacheEntry::new(value.clone(), self.cache_ttl);
            self.l1_cache.write().await.insert(key.to_string(), entry);
            
            if let Some(ref l2_cache) = self.l2_cache {
                let _ = l2_cache.set(key, &value).await;
            }
            
            return Ok(Some(serde_json::from_value(value)?));
        }
        self.metrics.record_l3_miss();
        
        Ok(None)
    }
    
    /// ìºì‹œì— ê°’ ì €ì¥ (Write-Behind)
    pub async fn set<T>(&self, key: &str, value: &T) -> Result<(), CacheError>
    where
        T: serde::Serialize,
    {
        let json_value = serde_json::to_value(value)?;
        
        // L1 ìºì‹œì— ì¦‰ì‹œ ì €ì¥ (ë©”ëª¨ë¦¬)
        let mut cache = self.l1_cache.write().await;
        if let Some(entry) = cache.get_mut(key) {
            entry.update(json_value.clone());
        } else {
            cache.insert(key.to_string(), CacheEntry::new(json_value.clone(), self.cache_ttl));
        }
        drop(cache);
        
        // ë³€ê²½ëœ í‚¤ë¡œ ë§ˆí‚¹ (ë‚˜ì¤‘ì— DB ë™ê¸°í™”)
        self.dirty_keys.lock().await.insert(key.to_string());
        
        debug!("ìºì‹œ ì €ì¥ë¨: {} (Write-Behind)", key);
        Ok(())
    }
    
    /// ìºì‹œì—ì„œ ê°’ ì‚­ì œ
    pub async fn delete(&self, key: &str) -> Result<(), CacheError> {
        // L1 ìºì‹œì—ì„œ ì‚­ì œ
        self.l1_cache.write().await.remove(key);
        
        // L2 ìºì‹œì—ì„œ ì‚­ì œ
        if let Some(ref l2_cache) = self.l2_cache {
            let _ = l2_cache.delete(key).await;
        }
        
        // L3 ìºì‹œì—ì„œ ì‚­ì œ
        self.l3_cache.delete(key).await?;
        
        // ë³€ê²½ëœ í‚¤ì—ì„œ ì œê±°
        self.dirty_keys.lock().await.remove(key);
        
        debug!("ìºì‹œ ì‚­ì œë¨: {}", key);
        Ok(())
    }
    
    /// ìºì‹œ ë¬´íš¨í™”
    pub async fn invalidate(&self, key: &str) -> Result<(), CacheError> {
        self.l1_cache.write().await.remove(key);
        
        if let Some(ref l2_cache) = self.l2_cache {
            let _ = l2_cache.delete(key).await;
        }
        
        debug!("ìºì‹œ ë¬´íš¨í™”ë¨: {}", key);
        Ok(())
    }
    
    /// ë§Œë£Œëœ ìºì‹œ ì—”íŠ¸ë¦¬ ì •ë¦¬
    async fn cleanup_expired_entries(l1_cache: &Arc<RwLock<HashMap<String, CacheEntry>>>) {
        let mut cache = l1_cache.write().await;
        cache.retain(|_, entry| !entry.is_expired());
    }
    
    /// ë³€ê²½ëœ í‚¤ë“¤ ë™ê¸°í™”
    async fn sync_dirty_keys(
        l1_cache: &Arc<RwLock<HashMap<String, CacheEntry>>>,
        l2_cache: &Option<Arc<dyn CacheLayer>>,
        l3_cache: &Arc<dyn CacheLayer>,
        dirty_keys: &Arc<Mutex<HashSet<String>>>,
        metrics: &Arc<CacheMetrics>,
    ) {
        let keys_to_sync = {
            let mut dirty_keys = dirty_keys.lock().await;
            dirty_keys.drain().collect::<Vec<_>>()
        };
        
        if keys_to_sync.is_empty() {
            return;
        }
        
        let cache = l1_cache.read().await;
        
        for key in keys_to_sync {
            if let Some(entry) = cache.get(&key) {
                // L2 ìºì‹œì— ë™ê¸°í™”
                if let Some(ref l2_cache) = l2_cache {
                    if let Err(e) = l2_cache.set(&key, &entry.value).await {
                        warn!("L2 ìºì‹œ ë™ê¸°í™” ì‹¤íŒ¨: {} - {}", key, e);
                    }
                }
                
                // L3 ìºì‹œì— ë™ê¸°í™”
                if let Err(e) = l3_cache.set(&key, &entry.value).await {
                    warn!("L3 ìºì‹œ ë™ê¸°í™” ì‹¤íŒ¨: {} - {}", key, e);
                    metrics.record_sync_error();
                } else {
                    metrics.record_sync_success();
                }
            }
        }
        
        debug!("ìºì‹œ ë™ê¸°í™” ì™„ë£Œ: {}ê°œ í‚¤", keys_to_sync.len());
    }
    
    /// ì„±ëŠ¥ ë©”íŠ¸ë¦­ ì¡°íšŒ
    pub fn get_metrics(&self) -> &CacheMetrics {
        &self.metrics
    }
    
    /// ì„±ëŠ¥ í†µê³„ ì¶œë ¥
    pub fn print_performance_stats(&self) {
        let metrics = &self.metrics;
        info!("ğŸ“Š Write-Behind ìºì‹œ ì„±ëŠ¥ í†µê³„:");
        info!("   L1 íˆíŠ¸ìœ¨: {:.2}%", metrics.get_l1_hit_rate());
        info!("   ì „ì²´ íˆíŠ¸ìœ¨: {:.2}%", metrics.get_overall_hit_rate());
        info!("   L1 íˆíŠ¸: {}", metrics.l1_hits.load(std::sync::atomic::Ordering::Relaxed));
        info!("   L1 ë¯¸ìŠ¤: {}", metrics.l1_misses.load(std::sync::atomic::Ordering::Relaxed));
        info!("   L2 íˆíŠ¸: {}", metrics.l2_hits.load(std::sync::atomic::Ordering::Relaxed));
        info!("   L2 ë¯¸ìŠ¤: {}", metrics.l2_misses.load(std::sync::atomic::Ordering::Relaxed));
        info!("   L3 íˆíŠ¸: {}", metrics.l3_hits.load(std::sync::atomic::Ordering::Relaxed));
        info!("   L3 ë¯¸ìŠ¤: {}", metrics.l3_misses.load(std::sync::atomic::Ordering::Relaxed));
        info!("   ë™ê¸°í™” ì„±ê³µ: {}", metrics.sync_operations.load(std::sync::atomic::Ordering::Relaxed));
        info!("   ë™ê¸°í™” ì‹¤íŒ¨: {}", metrics.sync_errors.load(std::sync::atomic::Ordering::Relaxed));
    }
    
    /// ìºì‹œ ìƒíƒœ ì¡°íšŒ
    pub async fn get_cache_status(&self) -> CacheStatus {
        let l1_size = self.l1_cache.read().await.len();
        let dirty_count = self.dirty_keys.lock().await.len();
        
        CacheStatus {
            l1_size,
            dirty_count,
            l2_enabled: self.l2_cache.is_some(),
        }
    }
}

/// ìºì‹œ ìƒíƒœ ì •ë³´
#[derive(Debug, Clone)]
pub struct CacheStatus {
    pub l1_size: usize,
    pub dirty_count: usize,
    pub l2_enabled: bool,
}

/// ì”ê³  ì „ìš© Write-Behind ìºì‹œ
pub struct BalanceCache {
    cache: WriteBehindCache,
}

impl BalanceCache {
    pub fn new(l3_cache: Arc<dyn CacheLayer>) -> Self {
        let cache = WriteBehindCache::new(
            l3_cache,
            Duration::from_millis(100), // 100msë§ˆë‹¤ ë™ê¸°í™”
            Duration::from_secs(300),   // 5ë¶„ TTL
        );
        
        Self { cache }
    }
    
    /// ì”ê³  ì¡°íšŒ (ë©”ëª¨ë¦¬ ìš°ì„ )
    pub async fn get_balance(&self, user_id: &str, asset: &str) -> Result<Option<u64>, CacheError> {
        let key = format!("balance:{}:{}", user_id, asset);
        self.cache.get::<u64>(&key).await
    }
    
    /// ì”ê³  ì—…ë°ì´íŠ¸ (Write-Behind)
    pub async fn update_balance(&self, user_id: &str, asset: &str, amount: u64) -> Result<(), CacheError> {
        let key = format!("balance:{}:{}", user_id, asset);
        self.cache.set(&key, &amount).await
    }
    
    /// ì”ê³  ì¦ê°€
    pub async fn add_balance(&self, user_id: &str, asset: &str, delta: u64) -> Result<u64, CacheError> {
        let current = self.get_balance(user_id, asset).await?.unwrap_or(0);
        let new_amount = current + delta;
        self.update_balance(user_id, asset, new_amount).await?;
        Ok(new_amount)
    }
    
    /// ì”ê³  ê°ì†Œ
    pub async fn subtract_balance(&self, user_id: &str, asset: &str, delta: u64) -> Result<Option<u64>, CacheError> {
        let current = self.get_balance(user_id, asset).await?.unwrap_or(0);
        if current >= delta {
            let new_amount = current - delta;
            self.update_balance(user_id, asset, new_amount).await?;
            Ok(Some(new_amount))
        } else {
            Ok(None) // ì”ê³  ë¶€ì¡±
        }
    }
    
    pub fn start(&mut self) {
        self.cache.start();
    }
    
    pub async fn stop(&mut self) {
        self.cache.stop().await;
    }
    
    pub fn get_metrics(&self) -> &CacheMetrics {
        self.cache.get_metrics()
    }
}

#[cfg(test)]
mod tests {
    use super::*;
    use std::time::Duration;
    
    // Mock ìºì‹œ ë ˆì´ì–´
    struct MockCacheLayer {
        data: std::sync::Mutex<std::collections::HashMap<String, serde_json::Value>>,
    }
    
    impl MockCacheLayer {
        fn new() -> Self {
            Self {
                data: std::sync::Mutex::new(std::collections::HashMap::new()),
            }
        }
    }
    
    #[async_trait::async_trait]
    impl CacheLayer for MockCacheLayer {
        async fn get(&self, key: &str) -> Result<Option<serde_json::Value>, CacheError> {
            Ok(self.data.lock().unwrap().get(key).cloned())
        }
        
        async fn set(&self, key: &str, value: &serde_json::Value) -> Result<(), CacheError> {
            self.data.lock().unwrap().insert(key.to_string(), value.clone());
            Ok(())
        }
        
        async fn delete(&self, key: &str) -> Result<(), CacheError> {
            self.data.lock().unwrap().remove(key);
            Ok(())
        }
        
        async fn exists(&self, key: &str) -> Result<bool, CacheError> {
            Ok(self.data.lock().unwrap().contains_key(key))
        }
    }
    
    #[tokio::test]
    async fn test_write_behind_cache_basic() {
        let l3_cache = Arc::new(MockCacheLayer::new());
        let mut cache = WriteBehindCache::new(
            l3_cache,
            Duration::from_millis(10),
            Duration::from_secs(1),
        );
        
        cache.start();
        
        // ê°’ ì €ì¥
        cache.set("test_key", &42u64).await.unwrap();
        
        // ê°’ ì¡°íšŒ
        let value: Option<u64> = cache.get("test_key").await.unwrap();
        assert_eq!(value, Some(42));
        
        // L1 íˆíŠ¸ í™•ì¸
        let metrics = cache.get_metrics();
        assert!(metrics.l1_hits.load(std::sync::atomic::Ordering::Relaxed) > 0);
        
        cache.stop().await;
    }
    
    #[tokio::test]
    async fn test_balance_cache() {
        let l3_cache = Arc::new(MockCacheLayer::new());
        let mut balance_cache = BalanceCache::new(l3_cache);
        
        balance_cache.start();
        
        // ì”ê³  ì„¤ì •
        balance_cache.update_balance("user1", "KRW", 1000000).await.unwrap();
        
        // ì”ê³  ì¡°íšŒ
        let balance = balance_cache.get_balance("user1", "KRW").await.unwrap();
        assert_eq!(balance, Some(1000000));
        
        // ì”ê³  ì¦ê°€
        let new_balance = balance_cache.add_balance("user1", "KRW", 500000).await.unwrap();
        assert_eq!(new_balance, 1500000);
        
        // ì”ê³  ê°ì†Œ
        let remaining = balance_cache.subtract_balance("user1", "KRW", 200000).await.unwrap();
        assert_eq!(remaining, Some(1300000));
        
        balance_cache.stop().await;
    }
}